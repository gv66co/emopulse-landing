<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>NEURO-CORE ELITE v3.0</title>
    <link href="https://fonts.googleapis.com/css2?family=Orbitron:wght@400;700&display=swap" rel="stylesheet">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/three.js/r128/three.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/@vladmandic/face-api/dist/face-api.js"></script>
    <style>
        :root {
            --deep-space: #000510;
            --neon-blue: #00e5ff;
            --aura-glow: rgba(0, 229, 255, 0.4);
            --ui-border: rgba(79, 195, 247, 0.3);
        }

        body, html {
            margin: 0; padding: 0; width: 100%; height: 100%;
            background-color: var(--deep-space); color: #81d4fa;
            font-family: 'Orbitron', sans-serif; overflow: hidden;
        }

        /* Three.js drobÄ— visame fone */
        #aura-canvas {
            display: block; position: fixed; top: 0; left: 0;
            z-index: 1; pointer-events: none;
        }

        /* HUD Elementai */
        #hud { position: absolute; inset: 0; z-index: 10; pointer-events: none; }

        .panel {
            position: absolute; background: rgba(0, 10, 30, 0.85);
            backdrop-filter: blur(15px); border: 1px solid var(--ui-border);
            border-left: 5px solid var(--neon-blue); padding: 20px;
            box-shadow: 0 0 20px var(--aura-glow);
            clip-path: polygon(0 0, 100% 0, 100% 80%, 90% 100%, 0 100%);
            pointer-events: auto;
        }

        #panel-l { top: 30px; left: 30px; width: 250px; }
        #panel-r { top: 30px; right: 30px; text-align: right; width: 250px; }

        .stat-label { font-size: 0.6rem; opacity: 0.6; letter-spacing: 2px; margin-bottom: 5px; }
        .stat-value { font-size: 1.1rem; color: #fff; text-shadow: 0 0 10px var(--neon-blue); margin-bottom: 15px; }

        /* Kameros langas */
        #video-container {
            position: absolute; bottom: 30px; right: 30px;
            width: 280px; height: 210px; border: 1px solid var(--neon-blue);
            z-index: 15; overflow: hidden; background: #000;
        }

        video {
            width: 100%; height: 100%; object-fit: cover;
            transform: scaleX(-1); filter: grayscale(1) brightness(0.8) contrast(1.2);
            opacity: 0.7;
        }

        #face-canvas {
            position: absolute; top: 0; left: 0; width: 100%; height: 100%;
            z-index: 20; transform: scaleX(-1); pointer-events: none;
        }

        #video-container::after {
            content: ""; position: absolute; top: 0; left: 0;
            width: 100%; height: 3px; background: var(--neon-blue);
            box-shadow: 0 0 15px var(--neon-blue); z-index: 25;
            animation: scan 3s linear infinite;
        }

        @keyframes scan { 0% { top: 0; } 100% { top: 100%; } }

        #ai-comms {
            position: absolute; bottom: 120px; left: 50%; transform: translateX(-50%);
            width: 60%; text-align: center;
            background: linear-gradient(90deg, transparent, rgba(0, 229, 255, 0.1), transparent);
            padding: 15px; border-bottom: 1px solid var(--neon-blue);
            font-size: 0.9rem; z-index: 30; text-transform: uppercase;
        }

        #scan-trigger {
            position: absolute; bottom: 50px; left: 50%; transform: translateX(-50%);
            z-index: 100; background: transparent; border: 1px solid var(--neon-blue);
            color: var(--neon-blue); padding: 12px 40px; font-family: 'Orbitron';
            cursor: pointer; transition: 0.3s; letter-spacing: 3px;
        }

        #scan-trigger:hover { background: var(--neon-blue); color: #000; box-shadow: 0 0 20px var(--neon-blue); }
        #scan-trigger:disabled { opacity: 0.3; cursor: wait; }
    </style>
</head>
<body>

<canvas id="aura-canvas"></canvas>

<div id="hud">
    <div id="panel-l" class="panel">
        <div class="stat-label">NEURAL LOAD</div>
        <div id="emo-val" class="stat-value">AWAITING</div>
        <div class="stat-label">HEART RATE</div>
        <div class="stat-value"><span id="bpm-val">--</span> BPM</div>
    </div>
    <div id="panel-r" class="panel">
        <div class="stat-label">SYSTEM STATUS</div>
        <div id="drift-val" class="stat-value">READY</div>
        <div class="stat-label">SYNC PROGRESS</div>
        <div id="prog-val" class="stat-value">0%</div>
    </div>
</div>

<div id="ai-comms">> INITIALIZE NEURAL LINK TO START BIOMETRIC ANALYSIS</div>
<button id="scan-trigger" onclick="initSystem()">START SCAN</button>

<div id="video-container">
    <video id="video-feed" autoplay muted playsinline></video>
    <canvas id="face-canvas"></canvas>
</div>

<script>
    let scene, camera, renderer, auraSphere, rings = [];
    let systemActive = false;
    let analysisData = [];
    const ANALYSIS_GOAL = 150; 
    const MODEL_URL = 'https://raw.githubusercontent.com/vladmandic/face-api/master/model/';

    const EMOTION_MAP = {
        neutral:  { color: 0x00e5ff, speed: 0.005 },
        happy:    { color: 0x00ff88, speed: 0.015 },
        sad:      { color: 0x0066ff, speed: 0.002 },
        angry:    { color: 0xff3300, speed: 0.040 },
        surprised:{ color: 0xffdd00, speed: 0.020 }
    };

    function speak(text) {
        window.speechSynthesis.cancel();
        const utter = new SpeechSynthesisUtterance(text);
        const voices = window.speechSynthesis.getVoices();
        const softVoice = voices.find(v => v.name.includes('Female') || v.name.includes('Zira'));
        if (softVoice) utter.voice = softVoice;
        utter.pitch = 1.1; utter.rate = 0.85;
        window.speechSynthesis.speak(utter);
        document.getElementById('ai-comms').innerText = "> IA: " + text;
    }

    function init3D() {
        const canvas = document.getElementById('aura-canvas');
        scene = new THREE.Scene();
        camera = new THREE.PerspectiveCamera(75, window.innerWidth / window.innerHeight, 0.1, 1000);
        renderer = new THREE.WebGLRenderer({ canvas: canvas, antialias: true, alpha: true });
        renderer.setSize(window.innerWidth, window.innerHeight);

        const geometry = new THREE.IcosahedronGeometry(2.2, 15);
        const material = new THREE.MeshPhongMaterial({ color: 0x00e5ff, wireframe: true, transparent: true, opacity: 0.4 });
        auraSphere = new THREE.Mesh(geometry, material);
        scene.add(auraSphere);

        for (let i = 0; i < 3; i++) {
            const rGeo = new THREE.TorusGeometry(3 + i * 0.6, 0.01, 16, 100);
            const rMat = new THREE.MeshBasicMaterial({ color: 0x7d2ae8, transparent: true, opacity: 0.2 });
            const ring = new THREE.Mesh(rGeo, rMat);
            rings.push(ring);
            scene.add(ring);
        }

        const light = new THREE.PointLight(0xffffff, 2);
        light.position.set(5, 5, 5);
        scene.add(light);
        scene.add(new THREE.AmbientLight(0x404040, 2));
        camera.position.z = 8;

        function animate() {
            requestAnimationFrame(animate);
            if(auraSphere) {
                auraSphere.rotation.y += 0.003;
                rings.forEach((r, i) => { r.rotation.z += 0.001 * (i + 1); });
            }
            renderer.render(scene, camera);
        }
        animate();
    }

    function updateAura(emotion, intensity) {
        if (!auraSphere) return;
        const theme = EMOTION_MAP[emotion] || EMOTION_MAP.neutral;
        auraSphere.material.color.lerp(new THREE.Color(theme.color), 0.05);
        const scale = 1 + (intensity * 0.5);
        auraSphere.scale.lerp(new THREE.Vector3(scale, scale, scale), 0.1);
        
        if (emotion === 'angry') {
            auraSphere.position.x = (Math.random() - 0.5) * 0.15;
        } else {
            auraSphere.position.x *= 0.9;
        }
    }

    function drawNeurolink(landmarks) {
        const ctx = document.getElementById('face-canvas').getContext('2d');
        ctx.clearRect(0, 0, ctx.canvas.width, ctx.canvas.height);
        const pts = landmarks.positions;
        ctx.strokeStyle = 'rgba(0, 229, 255, 0.3)';
        ctx.lineWidth = 0.5;
        ctx.beginPath();
        for (let i = 0; i < pts.length; i++) {
            for (let j = i + 1; j < pts.length; j++) {
                const d = Math.hypot(pts[i].x - pts[j].x, pts[i].y - pts[j].y);
                if (d < 25) {
                    ctx.moveTo(pts[i].x, pts[i].y);
                    ctx.lineTo(pts[j].x, pts[j].y);
                }
            }
        }
        ctx.stroke();
    }

    async function initSystem() {
        const btn = document.getElementById('scan-trigger');
        btn.disabled = true;
        document.getElementById('ai-comms').innerText = "> CONNECTING TO NEURAL INTERFACE...";

        try {
            await faceapi.nets.tinyFaceDetector.loadFromUri(MODEL_URL);
            await faceapi.nets.faceLandmark68Net.loadFromUri(MODEL_URL);
            await faceapi.nets.faceExpressionNet.loadFromUri(MODEL_URL);

            const stream = await navigator.mediaDevices.getUserMedia({ video: {} });
            document.getElementById('video-feed').srcObject = stream;

            const video = document.getElementById('video-feed');
            video.onloadedmetadata = () => {
                const canvas = document.getElementById('face-canvas');
                canvas.width = video.videoWidth;
                canvas.height = video.videoHeight;
                systemActive = true;
                runLoop();
                speak("Connection established. Analyzing bio-signature.");
            };
        } catch (e) {
            document.getElementById('ai-comms').innerText = "> ERROR: INITIALIZATION FAILED.";
            btn.disabled = false;
        }
    }

    function runLoop() {
        const video = document.getElementById('video-feed');
        setInterval(async () => {
            if (!systemActive) return;
            const res = await faceapi.detectSingleFace(video, new faceapi.TinyFaceDetectorOptions({ inputSize: 416 }))
                .withFaceLandmarks().withFaceExpressions();

            if (res) {
                drawNeurolink(res.landmarks);
                const top = Object.keys(res.expressions).reduce((a, b) => res.expressions[a] > res.expressions[b] ? a : b);
                const intensity = res.expressions[top];

                document.getElementById('emo-val').innerText = top.toUpperCase();
                document.getElementById('bpm-val').innerText = Math.round(68 + (res.expressions.angry + res.expressions.happy) * 32);
                
                analysisData.push(top);
                document.getElementById('prog-val').innerText = Math.round((analysisData.length / ANALYSIS_GOAL) * 100) + "%";
                document.getElementById('drift-val').innerText = intensity > 0.8 ? "HIGH INTENSITY" : "STABLE";

                updateAura(top, intensity);

                if (analysisData.length >= ANALYSIS_GOAL) {
                    performReport();
                    analysisData = [];
                }
            }
        }, 100);
    }

    function performReport() {
        const counts = {};
        analysisData.forEach(e => counts[e] = (counts[e] || 0) + 1);
        const dominant = Object.keys(counts).reduce((a, b) => counts[a] > counts[b] ? a : b);
        const reports = {
            happy: "Neural resonance is high. Sustained dopamine levels confirmed.",
            neutral: "Core stability maintained. Bio-rhythm is in perfect sync.",
            sad: "Minor frequency drop. Recalibrating emotional core.",
            angry: "Neural friction identified. Suggesting immediate stabilization.",
            surprised: "Heightened alertness. Neural field is expanding rapidly."
        };
        speak(reports[dominant] || "Analysis complete. Neural state confirmed.");
    }

    window.onload = () => { init3D(); window.speechSynthesis.getVoices(); };
</script>
</body>
</html>
