<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>NEURO-AURA DRIFT v1.0</title>
    <link href="https://fonts.googleapis.com/css2?family=Orbitron:wght@400;700&display=swap" rel="stylesheet">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/three.js/r128/three.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/@vladmandic/face-api/dist/face-api.js"></script>
    <style>
        :root { --neon: #00e5ff; --bg: #000510; }
        body { margin: 0; background: var(--bg); color: var(--neon); font-family: 'Orbitron', sans-serif; overflow: hidden; }
        #overlay { position: absolute; top: 20px; left: 20px; z-index: 100; background: rgba(0,20,40,0.8); padding: 20px; border-left: 4px solid var(--neon); border-radius: 0 10px 10px 0; pointer-events: none; }
        #video { position: absolute; bottom: 20px; right: 20px; width: 220px; border: 1px solid var(--neon); border-radius: 10px; z-index: 100; transform: scaleX(-1); filter: saturate(0) brightness(1.2); }
        #loader { position: fixed; inset: 0; background: #000; z-index: 1000; display: flex; align-items: center; justify-content: center; font-size: 1.5rem; text-transform: uppercase; letter-spacing: 5px; }
        .data-val { color: #fff; text-shadow: 0 0 10px var(--neon); }
    </style>
</head>
<body>

<div id="loader">Initializing Neural Links...</div>
<div id="overlay">
    <div>NEURAL STATUS: <span id="status" class="data-val">OFFLINE</span></div>
    <div>EMOTION DRIFT: <span id="emotion" class="data-val">WAITING</span></div>
    <div>PULSE (EST): <span id="pulse" class="data-val">0</span> BPM</div>
</div>
<video id="video" autoplay muted></video>

<script>
    let scene, camera, renderer, aura, particles;
    let currentEmotion = "neutral";
    let emotionIntensity = 0;

    // --- 1. INITIALIZE AI MODELS ---
    async function startSystem() {
        try {
            const MODEL_URL = 'https://cdn.jsdelivr.net/npm/@vladmandic/face-api/model/';
            await faceapi.nets.tinyFaceDetector.loadFromUri(MODEL_URL);
            await faceapi.nets.faceExpressionNet.loadFromUri(MODEL_URL);
            
            const stream = await navigator.mediaDevices.getUserMedia({ video: {} });
            document.getElementById('video').srcObject = stream;
            
            document.getElementById('loader').style.display = 'none';
            init3D();
            processEmotions();
        } catch (err) {
            document.getElementById('loader').innerText = "ERROR: ACCESS DENIED";
            console.error(err);
        }
    }

    // --- 2. 3D AURA DRIFT ENGINE ---
    function init3D() {
        scene = new THREE.Scene();
        camera = new THREE.PerspectiveCamera(75, window.innerWidth / window.innerHeight, 0.1, 1000);
        renderer = new THREE.WebGLRenderer({ antialias: true });
        renderer.setSize(window.innerWidth, window.innerHeight);
        document.body.appendChild(renderer.domElement);

        // Neural Body Geometry
        const geo = new THREE.IcosahedronGeometry(2, 10);
        const mat = new THREE.MeshPhongMaterial({
            color: 0x00e5ff, wireframe: true, emissive: 0x00e5ff, emissiveIntensity: 0.5
        });
        aura = new THREE.Mesh(geo, mat);
        scene.add(aura);

        // Space Particles
        const partGeo = new THREE.BufferGeometry();
        const pos = [];
        for(let i=0; i<10000; i++) pos.push((Math.random()-0.5)*20, (Math.random()-0.5)*20, (Math.random()-0.5)*20);
        partGeo.setAttribute('position', new THREE.Float32BufferAttribute(pos, 3));
        particles = new THREE.Points(partGeo, new THREE.PointsMaterial({color: 0xffffff, size: 0.02}));
        scene.add(particles);

        const light = new THREE.PointLight(0x00e5ff, 2, 100);
        light.position.set(10, 10, 10);
        scene.add(light);
        camera.position.z = 5;

        animate();
    }

    function animate() {
        requestAnimationFrame(animate);
        let speed = 0.005 + (emotionIntensity * 0.05);
        aura.rotation.y += speed;
        aura.rotation.x += speed * 0.5;
        
        // Aura reaction logic
        const scale = 1 + (emotionIntensity * 0.5);
        aura.scale.lerp(new THREE.Vector3(scale, scale, scale), 0.1);

        particles.rotation.y -= 0.001;
        renderer.render(scene, camera);
    }

    // --- 3. NEURAL DATA SYNC ---
    async function processEmotions() {
        const video = document.getElementById('video');
        setInterval(async () => {
            const detections = await faceapi.detectAllFaces(video, new faceapi.TinyFaceDetectorOptions()).withFaceExpressions();
            if (detections.length > 0) {
                const expressions = detections[0].expressions;
                currentEmotion = Object.keys(expressions).reduce((a, b) => expressions[a] > expressions[b] ? a : b);
                emotionIntensity = expressions[currentEmotion];

                document.getElementById('status').innerText = "CONNECTED";
                document.getElementById('emotion').innerText = currentEmotion.toUpperCase();
                document.getElementById('pulse').innerText = Math.floor(65 + (emotionIntensity * 30));

                // Change Color based on drift
                if(currentEmotion === 'happy') aura.material.color.setHex(0x00ff00);
                else if(currentEmotion === 'angry') aura.material.color.setHex(0xff0000);
                else aura.material.color.setHex(0x00e5ff);
            }
        }, 100);
    }

    window.onload = startSystem;
</script>
</body>
</html>
